{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再現ミス  \n",
    "more_title_pca  \n",
    "title_disc_pca  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-12-01T11:47:31.845974Z",
     "iopub.status.busy": "2020-12-01T11:47:31.845187Z",
     "iopub.status.idle": "2020-12-01T11:47:32.976594Z",
     "shell.execute_reply": "2020-12-01T11:47:32.975574Z"
    },
    "papermill": {
     "duration": 1.162226,
     "end_time": "2020-12-01T11:47:32.976747",
     "exception": false,
     "start_time": "2020-12-01T11:47:31.814521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import mean_squared_log_error, log_loss, roc_curve\n",
    "\n",
    "import lightgbm as lgb ##2.3.1\n",
    "import catboost\n",
    "from catboost import CatBoostRegressor, Pool, CatBoost\n",
    "import pickle\n",
    "import os\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 43\n",
    "\n",
    "def make_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "        \n",
    "EXP_ID = \"exp\"\n",
    "make_dir(\"../output/\" + EXP_ID)\n",
    "DATADIR = \"../input/\"\n",
    "OUTPUT_DIR = \"../output/\" + EXP_ID + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-12-01T11:47:33.023368Z",
     "iopub.status.busy": "2020-12-01T11:47:33.022578Z",
     "iopub.status.idle": "2020-12-01T11:47:33.097956Z",
     "shell.execute_reply": "2020-12-01T11:47:33.098849Z"
    },
    "papermill": {
     "duration": 0.102794,
     "end_time": "2020-12-01T11:47:33.099118",
     "exception": false,
     "start_time": "2020-12-01T11:47:32.996324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(DATADIR + \"train.csv\")\n",
    "test_data = pd.read_csv(DATADIR + \"test.csv\")\n",
    "\n",
    "all_df = pd.concat([train_data, test_data], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T11:47:33.469473Z",
     "iopub.status.busy": "2020-12-01T11:47:33.463617Z",
     "iopub.status.idle": "2020-12-01T11:47:33.473488Z",
     "shell.execute_reply": "2020-12-01T11:47:33.474167Z"
    },
    "papermill": {
     "duration": 0.041904,
     "end_time": "2020-12-01T11:47:33.474349",
     "exception": false,
     "start_time": "2020-12-01T11:47:33.432445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "def label_encoder(_df_input, cols):\n",
    "    \"\"\"\n",
    "    colsに対してLabelEncodingをする関数\n",
    "    \"\"\"\n",
    "    df_input = _df_input.copy()\n",
    "    df_out = pd.DataFrame()\n",
    "    \n",
    "    for c in cols:\n",
    "        df_input[c] = np.where(df_input[c].isnull(), \"NULL\", df_input[c])\n",
    "        le = LabelEncoder()\n",
    "        df_out[c] = le.fit_transform(df_input[c])\n",
    "        \n",
    "    return df_out\n",
    "\n",
    "def count_encoder(df_input, cols):\n",
    "    \"\"\"\n",
    "    colsに対してCountEncodingをする関数\n",
    "    \"\"\"\n",
    "    df_out = pd.DataFrame()\n",
    "    if type(cols) == str:\n",
    "        c = cols\n",
    "        dic = df_input[c].value_counts().to_dict()\n",
    "        df_out[c + \"CE\"] = df_input[c].map(dic)\n",
    "        return df_out\n",
    "    \n",
    "    for c in cols:\n",
    "        dic = df_input[c].value_counts().to_dict()\n",
    "        df_out[c + \"CE\"] = df_input[c].map(dic)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_feature(df_input):\n",
    "    \"\"\"\n",
    "    main tableのCountEncoding\n",
    "    \"\"\"\n",
    "    df_out = pd.DataFrame()\n",
    "    \n",
    "    cols = [\"principal_maker\", \"copyright_holder\", \"acquisition_method\", \"acquisition_credit_line\"]\n",
    "    cols += [\"art_series_id\"]\n",
    "    cols += [\"description\"] #0307\n",
    "    cols += [\"long_title\", \"principal_or_first_maker\", \"sub_title\", \"more_title\", \n",
    "             \"acquisition_date\", \"dating_presenting_date\", \"dating_period\"] #dating_year_early\n",
    "    \n",
    "    cols += [\"dating_year_early\", \"dating_year_late\"] #0313\n",
    "    for c in cols:\n",
    "        dic = df_input[c].value_counts().to_dict()\n",
    "        df_out[c + \"CE\"] = df_input[c].map(dic)\n",
    "        \n",
    "    assert len(df_out.columns) == len(set(df_out.columns))\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_process(l):\n",
    "    output_l = []\n",
    "    for w in l:\n",
    "        if w is np.nan:\n",
    "            continue\n",
    "        output_l.append(str(w))\n",
    "        \n",
    "    if len(output_l)==0:\n",
    "        return np.nan\n",
    "    \n",
    "    return \", \".join(output_l)\n",
    "\n",
    "\n",
    "\n",
    "def merge_tables(df_input):\n",
    "    \"\"\"\n",
    "    複数のtableを結合する関数\n",
    "    \"\"\"\n",
    "    material = pd.read_csv(DATADIR + \"material.csv\").groupby(\"object_id\")[\"name\"].apply(list).apply(lambda x:\", \".join(sorted(x))).rename(\"material\")\n",
    "    tech = pd.read_csv(DATADIR + \"technique.csv\").groupby(\"object_id\")[\"name\"].apply(list).apply(lambda x:\", \".join(sorted(x))).rename(\"tech\")\n",
    "    obj_type = pd.read_csv(DATADIR + \"object_collection.csv\").groupby(\"object_id\")[\"name\"].apply(list).apply(lambda x:\", \".join(sorted(x))).rename(\"obj_type\")\n",
    "    person = pd.read_csv(DATADIR + \"historical_person.csv\").groupby(\"object_id\")[\"name\"].apply(list).apply(lambda x:\", \".join(sorted(x))).rename(\"person\")\n",
    "    place = pd.read_csv(DATADIR + \"production_place.csv\").groupby(\"object_id\")[\"name\"].apply(list).apply(lambda x:\", \".join(sorted(x))).rename(\"place\")\n",
    "    \n",
    "    _maker = pd.read_csv(DATADIR + \"principal_maker.csv\")\n",
    "    maker_occu = pd.read_csv(DATADIR + \"principal_maker_occupation.csv\").groupby(\"id\")[\"name\"].apply(list).apply(lambda x:\", \".join(sorted(x)))\n",
    "    maker_occu = maker_occu.rename(\"occu\")\n",
    "    _maker = pd.merge(_maker, maker_occu, on=\"id\", how=\"left\").drop(\"id\", axis=1)\n",
    "    \n",
    "    cols = ['qualification', 'roles', 'productionPlaces', 'maker_name', \"occu\"]\n",
    "    maker = pd.concat([_maker.groupby(\"object_id\")[c].apply(list).apply(lambda x:list_process(x)) for c in cols], axis=1)\n",
    "    \n",
    "    tables = pd.concat([material, tech,\n",
    "                        obj_type, person, place, maker], axis=1).reset_index().rename(columns={\"index\":\"object_id\"})\n",
    "\n",
    "    tables_out = pd.merge(df_input, tables, on=\"object_id\", how=\"left\")\n",
    "    return tables_out\n",
    "\n",
    "\n",
    "\n",
    "def tables_onehot(df_input):\n",
    "    \"\"\"\n",
    "    複数tableの要素をOneHot\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    for name in [\"material\", \"technique\", \"object_collection\", \"historical_person\", \"production_place\"]:\n",
    "        _df = pd.read_csv(DATADIR + f\"{name}.csv\")\n",
    "        if name == \"production_place\":\n",
    "            _df[\"name\"] = _df[\"name\"].apply(lambda x: x.replace(\"? \", \"\"))\n",
    "        outputs.append(pd.crosstab(_df[\"object_id\"], _df[\"name\"]).add_prefix(f\"{name}_OH_\"))\n",
    "    outputs = pd.concat(outputs, axis=1)\n",
    "    \n",
    "    idx = outputs.sum(axis=0) > 25 #30\n",
    "    _df_out = outputs[idx[idx].index]\n",
    "    _df_out = _df_out.reset_index().rename(columns={\"index\":\"object_id\"})\n",
    "    cols = _df_out.columns\n",
    "    df_out = pd.merge(df_input, _df_out, on=\"object_id\", how=\"left\")[[c for c in cols if c!=\"object_id\"]]\n",
    "    \n",
    "    assert df_input.shape[0] == df_out.shape[0]\n",
    "    df_out.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in df_out.columns]\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def null_features(df_input):\n",
    "    \"\"\"\n",
    "    全tableにおいてNaNになっている要素についてOneHot化\n",
    "    SVDで次元削減して特徴化\n",
    "    \"\"\"\n",
    "    df_out = pd.DataFrame()\n",
    "    tables = merge_tables(df_input)\n",
    "    df_out[\"num_null\"] = tables.isnull().sum(axis=1)\n",
    "    \n",
    "    for name in [\"color\", \"palette\"]:\n",
    "        color_df = pd.read_csv(DATADIR + f\"{name}.csv\")\n",
    "        color_ids = color_df[\"object_id\"].unique()\n",
    "        tables[name] = np.where(df_input[\"object_id\"].isin(color_ids),\n",
    "                                1,\n",
    "                                np.nan)\n",
    "    \n",
    "    tables = pd.concat([tables, \n",
    "                        subtitle_feature(all_df)[[\"h\", \"w\", \"d\", \"l\", \"weight\"]]], axis=1)\n",
    "    \n",
    "    cols = tables.columns[tables.isnull().astype(int).sum(axis=0) > 10]\n",
    "    cols = [c for c in cols if c not in [\"dating_sorting_date\", \"dating_year_early\"]]\n",
    "    \n",
    "    print(len(cols))\n",
    "    \n",
    "    null_df = tables[cols].isnull().astype(int)\n",
    "    \n",
    "    pipeline_ = Pipeline([\n",
    "        ('svd', TruncatedSVD(n_components=10, n_iter=30, random_state=42)), #5\n",
    "    ])\n",
    "    pipeline_.fit(null_df[:len(train_data)])\n",
    "    arr = pipeline_.transform(null_df)\n",
    "    df_out = pd.concat([df_out, \n",
    "                        pd.DataFrame(arr).add_prefix(f\"null_svd\")], axis=1)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lazy_merge_tables_feature(df_input):\n",
    "    \"\"\"\n",
    "    全テーブルの要素について文字長特徴\n",
    "    \"\"\"\n",
    "    df_out = pd.DataFrame()\n",
    "    _df = merge_tables(df_input)\n",
    "    \n",
    "    for c in [\"tech\", \"material\", \"obj_type\", \"place\", \"person\"]:\n",
    "        df_out[f\"{c}_len\"] = _df[c].str.len()\n",
    "        \n",
    "        dic = _df[c].value_counts().to_dict()\n",
    "        df_out[f\"{c}_CE\"] = _df[c].map(dic)\n",
    "\n",
    "    for c in [\"qualification\", \"roles\", \"maker_name\", \"occu\"]:\n",
    "        df_out[f\"{c}_len\"] = _df[c].str.len()\n",
    "        \n",
    "        dic = _df[c].value_counts().to_dict()\n",
    "        df_out[f\"{c}_CE\"] = _df[c].map(dic)\n",
    "        \n",
    "    \n",
    "    _df = _df.drop([\"object_id\", \"art_series_id\",\n",
    "                    \"dating_sorting_date\", \"dating_period\", \"dating_year_early\", \"dating_year_late\",\n",
    "                    \"likes\"], \n",
    "                    axis=1).fillna(\"\")\n",
    "    \n",
    "    tmp_df = pd.DataFrame()\n",
    "    for c in _df.columns:\n",
    "        tmp_df[c] = _df[c].astype(str).str.len()\n",
    "        \n",
    "    df_out[\"all_text_len\"] = tmp_df.sum(axis=1)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.models import word2vec, KeyedVectors\n",
    "#word2vec\n",
    "def resnet50_feature(df_input):\n",
    "    \"\"\"\n",
    "    ratioにそってrandom patternで画像生成(adversarial attackみたいな画像)\n",
    "    それに対してresnet50で特徴抽出したcsvを結合しPCA\n",
    "    \"\"\"\n",
    "    emb_df = pd.read_pickle(\"../feature/ResNet50_palette_embedding1000.pkl\")\n",
    "    \n",
    "    cols = [c for c in emb_df.columns if c!=\"object_id\"]\n",
    "    pca = PCA(n_components=3, random_state=42)\n",
    "    arr = pca.fit_transform(emb_df[cols].values)\n",
    "    #pca.fit(emb_df[cols][:len(train_data)].values)\n",
    "    #arr = pca.transform(emb_df[cols].values)\n",
    "    \n",
    "    _df_out = pd.DataFrame(arr).add_prefix(\"random_resnet\")\n",
    "    output_cols = _df_out.columns\n",
    "    \n",
    "    _df_out = pd.concat([emb_df[\"object_id\"], _df_out], axis=1)\n",
    "    \n",
    "    df_out = pd.merge(df_input, _df_out, on=\"object_id\", how=\"left\")[output_cols]\n",
    "    assert df_input.shape[0] == df_out.shape[0]\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorsys\n",
    "def _agg_df(df_input, color, func, prefix):\n",
    "    \"\"\"\n",
    "    palette_feature(下) 内部で用いる関数\n",
    "    HSV等を計算しratioを掛けて重み付き和\n",
    "    \n",
    "    func : HSVなどを算出する関数 (e.g. colorsys.rgb_to_hsv)\n",
    "    \"\"\"\n",
    "    arr = color.apply(lambda x:list(func(x[1], x[2], x[3])), axis=1).values.tolist()\n",
    "    _df = pd.concat([color[[\"object_id\", \"percentage\"]], pd.DataFrame(arr, columns=[\"val1\", \"val2\", \"val3\"])], axis=1)\n",
    "    \n",
    "    for c in [\"val1\", \"val2\", \"val3\"]:\n",
    "        _df[c] *= _df[\"percentage\"]\n",
    "    _df = _df.groupby(\"object_id\").sum()\n",
    "    tmp_df = pd.merge(df_input, _df, on=\"object_id\", how=\"left\")[[\"val1\", \"val2\", \"val3\"]].add_prefix(prefix)\n",
    "    \n",
    "    return tmp_df\n",
    "\n",
    "\n",
    "def palette_feature(df_input):\n",
    "    \"\"\" \n",
    "    HSL, HSV, YIQの計算　-> ratioを掛けて重み付き和\n",
    "    \"\"\"\n",
    "    df_out = pd.DataFrame()\n",
    "    palette = pd.read_csv(DATADIR + \"palette.csv\")\n",
    "    palette = palette.rename(columns={\"ratio\":\"percentage\",\n",
    "                                      \"color_r\":\"R\",\n",
    "                                      \"color_g\":\"G\",\n",
    "                                      \"color_b\":\"B\"})\n",
    "    for c in [\"R\", \"G\", \"B\"]:\n",
    "        palette[c] /= 255\n",
    "    df_out = [_agg_df(df_input, palette, colorsys.rgb_to_hls, \"HLS_\"),\n",
    "              _agg_df(df_input, palette, colorsys.rgb_to_hsv, \"HSV_\"),\n",
    "              _agg_df(df_input, palette, colorsys.rgb_to_yiq, \"YIQ_\")]\n",
    "    df_out = pd.concat(df_out, axis=1).iloc[:, 1:]\n",
    "    df_out = df_out.add_prefix(\"palette_\")\n",
    "    \n",
    "    assert df_input.shape[0] == df_out.shape[0]\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExG_agg_feature(df_input):\n",
    "    \"\"\" \n",
    "    ExG (and ExR, ExB) ベースのpalette特徴\n",
    "    \n",
    "    df_out1 : 各要素に対してExGを計算して, ratioは使わずobject_id単位で統計量計算\n",
    "    df_out2 : 各要素に対してExGを計算して, ratioを使ってobject_id単位で統計量計算\n",
    "    df_out3 : 各要素に対してExGを計算して, ExRとExGの値が0以上となったものだけを抽出, ratioを掛けて重み付き和\n",
    "    (ExBは分布の平均値が小さく不採用, 閾値を変えると効いた可能性は高い)\n",
    "    (また重み付き和だけでなく，ratioの和も効く気がする)\n",
    "    df_out4 : 各要素に対してgamma_grayを計算して, object_id単位で統計量\n",
    "    \"\"\"\n",
    "    palette = pd.read_csv(DATADIR + \"palette.csv\")\n",
    "    for c in ['color_r', 'color_g', 'color_b']:\n",
    "        palette[c] = palette[c].astype(float)\n",
    "    df_out = pd.DataFrame()\n",
    "    palette = palette.copy()\n",
    "    \n",
    "    palette[\"RGBsum\"] = palette.filter(regex=\"color_\").sum(axis=1)\n",
    "    palette[\"ExR\"] = (2*palette[\"color_r\"] - palette[\"color_g\"] - palette[\"color_b\"]) / palette[\"RGBsum\"]\n",
    "    palette[\"ExG\"] = (-palette[\"color_r\"] + 2*palette[\"color_g\"] - palette[\"color_b\"]) / palette[\"RGBsum\"]\n",
    "    palette[\"ExB\"] = (-palette[\"color_r\"] - palette[\"color_g\"] + 2*palette[\"color_b\"]) / palette[\"RGBsum\"]\n",
    "    \n",
    "    cols = [\"ExR\", \"ExG\", \"ExB\"]\n",
    "    df_out1 = palette.groupby(\"object_id\").agg([\"mean\", \"std\", \"max\", \"min\"])[[\"RGBsum\", \"ExR\", \"ExG\", \"ExB\"]]\n",
    "    df_out1.columns = [\"_\".join(c) for c in df_out1.columns]\n",
    "    \n",
    "    tmp_df = palette[[\"ratio\", \"object_id\", \"RGBsum\", \"ExR\", \"ExG\", \"ExB\"]].copy()\n",
    "    for c in [\"RGBsum\", \"ExR\", \"ExG\", \"ExB\"]:\n",
    "        tmp_df[c] *= tmp_df[\"ratio\"]\n",
    "    df_out2 = tmp_df.groupby(\"object_id\").agg([\"mean\", \"std\", \"max\", \"min\"])[[\"RGBsum\", \"ExR\", \"ExG\", \"ExB\"]]\n",
    "    df_out2.columns = [\"_\".join(c) + \"_prod_ratio\" for c in df_out2.columns]\n",
    "    \n",
    "    \n",
    "    df_out3 = []\n",
    "    for c in [\"ExR\", \"ExG\"]:#, \"ExB\"]:\n",
    "        tmp_df = palette[palette[c] > 0][[\"object_id\", \"ratio\", c]]\n",
    "        tmp_df[f\"ratio_count_{c}\"] = tmp_df[\"ratio\"] * tmp_df[c]\n",
    "        df_out3.append(\n",
    "            pd.merge(df_input, \n",
    "                     tmp_df.groupby(\"object_id\")[f\"ratio_count_{c}\"].sum(),\n",
    "                     on=\"object_id\", how=\"left\"\n",
    "                    )[f\"ratio_count_{c}\"].fillna(0)\n",
    "        )\n",
    "    df_out3 = pd.concat(df_out3, axis=1)\n",
    "    \n",
    "    \n",
    "    tmp_df = palette.copy()\n",
    "    tmp_df[\"gamma_gray\"] = tmp_df[\"color_r\"]*0.299 + tmp_df[\"color_g\"]*0.587 + tmp_df[\"color_b\"]*0.114\n",
    "    #tmp_df[\"r_gamma_gray\"] = tmp_df[\"gamma_gray\"] * tmp_df[\"ratio\"]\n",
    "\n",
    "    df_out4 = pd.concat([tmp_df.groupby(\"object_id\")[\"gamma_gray\"].agg([\"max\", \"min\", \"mean\"]).add_suffix(\"_gamma_gray\"),\n",
    "                        #tmp_df.groupby(\"object_id\")[\"r_gamma_gray\"].agg([\"max\", \"min\", \"sum\"]).add_suffix(\"_r\") #相関高杉\n",
    "                       ], axis=1)\n",
    "    \n",
    "    df_out = pd.concat([pd.merge(df_input, df_out1, on=\"object_id\", how=\"left\")[df_out1.columns],\n",
    "                        pd.merge(df_input, df_out2, on=\"object_id\", how=\"left\")[df_out2.columns],\n",
    "                        df_out3,\n",
    "                        pd.merge(df_input, df_out4, on=\"object_id\", how=\"left\")[df_out4.columns]\n",
    "                       ], axis=1)\n",
    "    \n",
    "    assert df_input.shape[0] == df_out.shape[0]\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_palette_agg(df_input):\n",
    "    \"\"\"\n",
    "    [１つの色(代表色)にまとめた上での特徴量]\n",
    "    rgb*ratioしてobject_id単位でsum\n",
    "    - 各成分(rgb)の値\n",
    "    - 各成分(rgb)の割合 r/r+g+b ...\n",
    "    - HLS\n",
    "    \"\"\"\n",
    "    df_out = pd.DataFrame()\n",
    "    palette = pd.read_csv(DATADIR + \"palette.csv\")\n",
    "    for c in [\"color_r\", \"color_g\", \"color_b\"]:\n",
    "        palette[c] *= palette[\"ratio\"]\n",
    "\n",
    "    agg_df = palette.groupby(\"object_id\").sum()\n",
    "    for c in [\"color_r\", \"color_g\", \"color_b\"]:\n",
    "        dic = agg_df[c].to_dict()\n",
    "        df_out[\"agg_\" + c] = df_input[\"object_id\"].map(dic)\n",
    "    \n",
    "    for c in [\"agg_color_r\", \"agg_color_g\", \"agg_color_b\"]:\n",
    "        df_out[c + \"_ratio_in_rgb\"] = df_out[c] / df_out.filter(regex=\"agg_color_\").sum(axis=1)    \n",
    "\n",
    "    palette[\"hls\"] = (palette.filter(regex=\"color\").max(axis=1) + palette.filter(regex=\"color\").min(axis=1)) / 2\n",
    "    dic = palette.groupby(\"object_id\")[\"hls\"].sum().to_dict()\n",
    "    df_out[\"sum_rHLS\"] = df_input[\"object_id\"].map(dic)\n",
    "    \n",
    "    assert df_input.shape[0] == df_out.shape[0]\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def lazy_ratio_agg(df_input):\n",
    "    \"\"\"\n",
    "    ratioのobject_id単位で統計量\n",
    "    \"\"\"\n",
    "    palette = pd.read_csv(DATADIR + \"palette.csv\")\n",
    "    df_out = pd.merge(df_input, \n",
    "                      palette.groupby(\"object_id\")[\"ratio\"].agg([\"max\", \"min\", \"std\"]),\n",
    "                      on=\"object_id\", how=\"left\")[[\"max\", \"min\", \"std\"]].add_suffix(\"_ratio\")\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import circmean\n",
    "def calc_circmean(df_input):\n",
    "    \"\"\"\n",
    "    HSVのHに対して\n",
    "    (最終subには使っていない)\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for idx_label, idx_count in zip(df_input[\"H\"], df_input[\"percentage\"]*10000):\n",
    "        data += [idx_label] * int(idx_count)\n",
    "    mean = circmean(data, high = 1, low = 0)\n",
    "    std = np.where(abs(df_input[\"H\"] - mean) > 0.5,\n",
    "                   1 - abs(df_input[\"H\"] - mean),\n",
    "                   abs(df_input[\"H\"] - mean)\n",
    "                  ).sum()\n",
    "    \n",
    "    return np.array([mean, std, df_input[\"V\"].max(), df_input[\"V\"].std(), (df_input[\"S\"]*df_input[\"V\"]).max()])\n",
    "\n",
    "\n",
    "def palette_HSV(df_input):\n",
    "    \"\"\"\n",
    "    HSV特化の特徴\n",
    "    ratioあり\n",
    "    (最終subには使っていない)\n",
    "    \"\"\"\n",
    "    palette = pd.read_csv(DATADIR + \"palette.csv\")\n",
    "    palette = palette.rename(columns={\"ratio\":\"percentage\",\n",
    "                                      \"color_r\":\"R\",\n",
    "                                      \"color_g\":\"G\",\n",
    "                                      \"color_b\":\"B\"})\n",
    "    for c in [\"R\", \"G\", \"B\"]:\n",
    "        palette[c] /= 255\n",
    "\n",
    "    arr = palette.apply(lambda x:list(colorsys.rgb_to_hsv(x[1], x[2], x[3])), axis=1).values.tolist()\n",
    "    _df = pd.concat([palette[[\"object_id\", \"percentage\"]], pd.DataFrame(arr, columns=[\"H\", \"S\", \"V\"])], axis=1)\n",
    "\n",
    "    df_out = pd.DataFrame(_df.groupby(\"object_id\").apply(calc_circmean).values.tolist(),\n",
    "                         columns=[\"Hcirc_mean\", \"Hcirc_std\", \"S_max\", \"S_std\", \"SV_prod\"])\n",
    "    df_out[\"object_id\"] = _df.groupby(\"object_id\").size().index\n",
    "    \n",
    "    cols = [c for c in df_out.columns if c != \"object_id\"]\n",
    "    _df_out = pd.merge(df_input, df_out, how=\"left\", on=\"object_id\")[cols]\n",
    "    \n",
    "    assert df_input.shape[0] == _df_out.shape[0]\n",
    "    return _df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageColor\n",
    "import colorsys\n",
    "\n",
    "def _agg_df_for_color(df_input, color, func, prefix):\n",
    "    \"\"\"\n",
    "    color.csvに対してHSV等を計算し，percentageで重み付き和\n",
    "    \"\"\"\n",
    "    arr = color.apply(lambda x:list(func(x[3], x[4], x[5])), axis=1).values.tolist()\n",
    "    _df = pd.concat([color[[\"object_id\", \"percentage\"]], pd.DataFrame(arr, columns=[\"R\", \"G\", \"B\"])], axis=1)\n",
    "    \n",
    "    for c in [\"R\", \"G\", \"B\"]:\n",
    "        _df[c] *= _df[\"percentage\"]\n",
    "    _df = _df.groupby(\"object_id\").sum()\n",
    "    tmp_df = pd.merge(df_input, _df, on=\"object_id\", how=\"left\")[[\"R\", \"G\", \"B\"]].add_prefix(prefix)\n",
    "    \n",
    "    return tmp_df\n",
    "\n",
    "\n",
    "def color_feature(df_input):\n",
    "    \"\"\"\n",
    "    color.csvに対してHSV等の計算\n",
    "    \"\"\"\n",
    "    df_out = pd.DataFrame()\n",
    "    color = pd.read_csv(DATADIR + \"color.csv\")\n",
    "    color = color.sort_values([\"object_id\", \"percentage\"]).reset_index(drop=True)\n",
    "\n",
    "    color = pd.concat([color,\n",
    "                       pd.DataFrame(color['hex'].str.strip().map(ImageColor.getrgb).values.tolist(), columns=['R', 'G', 'B'])\n",
    "                      ], axis=1)\n",
    "    color[\"percentage\"] /= 100\n",
    "    for c in [\"R\", \"G\", \"B\"]:\n",
    "        color[c] /= 255\n",
    "    \n",
    "    df_out = [_agg_df_for_color(df_input, color, colorsys.rgb_to_hls, \"HLS_\"),\n",
    "              _agg_df_for_color(df_input, color, colorsys.rgb_to_hsv, \"HSV_\"),\n",
    "              _agg_df_for_color(df_input, color, colorsys.rgb_to_yiq, \"YIQ_\")]\n",
    "    df_out = pd.concat(df_out, axis=1)\n",
    "    assert df_input.shape[0] == df_out.shape[0]\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_agg_feature(df_input):\n",
    "    \"\"\"\n",
    "    color.csvの単純なobject_id単位での統計量\n",
    "    \"\"\"\n",
    "    df_out = pd.DataFrame()\n",
    "    color = pd.read_csv(DATADIR + \"color.csv\")\n",
    "    agg_df = color.groupby(\"object_id\")[\"percentage\"].agg([\"max\", \"std\", \"min\", \"nunique\"])\n",
    "    for c in agg_df.columns:\n",
    "        dic = agg_df[c].to_dict()\n",
    "        df_out[\"color\" + c] =  df_input[\"object_id\"].map(dic)\n",
    "    \n",
    "    assert df_input.shape[0] == df_out.shape[0]\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_ExG_agg_feature(df_input):\n",
    "    \"\"\" \n",
    "    ExG_agg_featureをcolor.csvに適応\n",
    "    df_out1 : percentage掛けず統計量\n",
    "    df_out2 : percentage掛けて統計量\n",
    "    \"\"\"\n",
    "    color = pd.read_csv(DATADIR + \"color.csv\")\n",
    "    color[\"ratio\"] = color[\"percentage\"] / 100\n",
    "    palette = pd.concat([color,\n",
    "                       pd.DataFrame(color['hex'].str.strip().map(ImageColor.getrgb).values.tolist(), columns=['color_r', 'color_g', 'color_b'])\n",
    "                        ], axis=1)\n",
    "    df_out = pd.DataFrame()\n",
    "    \n",
    "    palette[\"RGBsum\"] = palette.filter(regex=\"color_\").sum(axis=1)\n",
    "    palette[\"ExR\"] = (2*palette[\"color_r\"] - palette[\"color_g\"] - palette[\"color_b\"]) / palette[\"RGBsum\"]\n",
    "    palette[\"ExG\"] = (-palette[\"color_r\"] + 2*palette[\"color_g\"] - palette[\"color_b\"]) / palette[\"RGBsum\"]\n",
    "    palette[\"ExB\"] = (-palette[\"color_r\"] - palette[\"color_g\"] + 2*palette[\"color_b\"]) / palette[\"RGBsum\"]\n",
    "    \n",
    "    cols = [\"ExR\", \"ExG\", \"ExB\"]\n",
    "    df_out1 = palette.groupby(\"object_id\").agg([\"mean\", \"std\", \"max\", \"min\"])[[\"RGBsum\", \"ExR\", \"ExG\", \"ExB\"]]\n",
    "    df_out1.columns = [\"_\".join(c) for c in df_out1.columns]\n",
    "    \n",
    "    tmp_df = palette[[\"ratio\", \"object_id\", \"RGBsum\", \"ExR\", \"ExG\", \"ExB\"]].copy()\n",
    "    for c in [\"RGBsum\", \"ExR\", \"ExG\", \"ExB\"]:\n",
    "        tmp_df[c] *= tmp_df[\"ratio\"]\n",
    "    df_out2 = tmp_df.groupby(\"object_id\").agg([\"mean\", \"std\", \"max\", \"min\"])[[\"RGBsum\", \"ExR\", \"ExG\", \"ExB\"]]\n",
    "    df_out2.columns = [\"_\".join(c) + \"_prod_ratio\" for c in df_out2.columns]\n",
    "    df_out = pd.concat([pd.merge(df_input, df_out1, on=\"object_id\", how=\"left\")[df_out1.columns],\n",
    "                        pd.merge(df_input, df_out2, on=\"object_id\", how=\"left\")[df_out2.columns],\n",
    "                       ], axis=1)\n",
    "    \n",
    "    assert df_input.shape[0] == df_out.shape[0]\n",
    "    return df_out.add_suffix(\"_color\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fasttext import load_model\n",
    "\n",
    "def lang_detect(df_input):\n",
    "    \"\"\"\n",
    "    Arai-san discussionのfasttext言語判定モデル\n",
    "    lid.176.bin は https://fasttext.cc/docs/en/language-identification.html\n",
    "    \"\"\"\n",
    "    model = load_model(\"../feature/lid.176.bin\")\n",
    "    df_out = pd.DataFrame()\n",
    "    for c in [\"title\", \"principal_maker\", \"description\", \"long_title\"]:\n",
    "        null_idx = df_input[c].isnull()\n",
    "        \n",
    "        df_out[f\"{c}_langlabel\"] = np.where(null_idx,\n",
    "                                            np.nan,\n",
    "                                            df_input.fillna(\"NULL\")[c].apply(lambda x:x.replace(\"\\n\", \" \")).apply(lambda x:model.predict(x)[0][0])\n",
    "                                           )\n",
    "        \n",
    "        df_out[f\"{c}_langlabel_prob\"] = np.where(null_idx,\n",
    "                                            np.nan,\n",
    "                                            df_input.fillna(\"NULL\")[c].apply(lambda x:x.replace(\"\\n\", \" \")).apply(lambda x:model.predict(x)[1][0])\n",
    "                                           )\n",
    "        \n",
    "        dic = df_out[f\"{c}_langlabel\"].value_counts().to_dict()\n",
    "        df_out[f\"{c}_langlabel\"] = df_out[f\"{c}_langlabel\"].map(dic)\n",
    "        \n",
    "    assert df_input.shape[0] == df_out.shape[0]\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_description(df_input):\n",
    "    bert_feat = pd.read_csv(\"../feature/nl_en_BERT_transed_description.csv\")\n",
    "    feat_df = pd.merge(df_input, bert_feat, on=\"object_id\", how=\"left\")\n",
    "\n",
    "    cols = [c for c in bert_feat.columns if c != 'object_id']\n",
    "    feat_df = feat_df[cols]\n",
    "    pca = PCA(n_components=50, random_state=42)\n",
    "    \n",
    "    arr = pca.fit_transform(feat_df.values)\n",
    "    df_out1 = pd.DataFrame(arr).add_prefix(\"nl_en_bert_desc_pca\")\n",
    "    \n",
    "    \n",
    "    bert_feat = pd.read_csv(\"../feature/nl_en_BERT_transed_long_title.csv\")\n",
    "    feat_df = pd.merge(df_input, bert_feat, on=\"object_id\", how=\"left\")\n",
    "    \n",
    "    cols = [c for c in bert_feat.columns if c != 'object_id']\n",
    "    feat_df = feat_df[cols]\n",
    "    pca = PCA(n_components=64, random_state=42)\n",
    "    arr = pca.fit_transform(feat_df.values)\n",
    "    df_out2 = pd.DataFrame(arr).add_prefix(\"nl_en_bert_long_title_pca\")\n",
    "    df_out = pd.concat([df_out1, df_out2], axis=1)\n",
    "    \n",
    "    assert df_input.shape[0] == df_out.shape[0]\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, TruncatedSVD, SparsePCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def maker_countvectorizer(df_input):\n",
    "    \"\"\"\n",
    "    principal_makerでCountVectorizer\n",
    "    \n",
    "    ngramは(1,1)が良い\n",
    "    max_feature指定したものの精度が悪化した．出現回数が少ないことの表現を奪った説がある(大丈夫な説もある)\n",
    "    countなので[train, test]でfitしても良いかも\n",
    "    \"\"\"\n",
    "    #vectorizer = CountVectorizer(max_features=3200, ngram_range=(1, 2)) #max_feature指定したものの数が少ないことの表現を奪った説がある(大丈夫な説もある)\n",
    "    vectorizer = CountVectorizer(max_features=None, ngram_range=(1, 1))\n",
    "    arr = vectorizer.fit_transform(df_input[\"principal_maker\"].dropna()).toarray()\n",
    "    pca = PCA(n_components=50, random_state=42) #pcaでした\n",
    "    arr = pca.fit_transform(arr)\n",
    "    df_out = pd.DataFrame(arr).add_prefix(\"maker_svd\")\n",
    "    assert df_input.shape[0] == df_out.shape[0]\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def description_tfidfvectorizer(df_input):\n",
    "    \"\"\"\n",
    "    descriptionのTfidf (text cleaningはしない)\n",
    "    \"\"\"\n",
    "    pipeline_ = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(ngram_range=(1, 2))), #(1,2) #n_gram足しちゃいました\n",
    "        ('svd', TruncatedSVD(n_components=64, n_iter=10, random_state=42)),\n",
    "    ])\n",
    "    c = \"description\"\n",
    "    pipeline_.fit(df_input[c][:len(train_data)].fillna(\"NULL\"))\n",
    "    arr = pipeline_.transform(df_input[c].fillna(\"NULL\"))\n",
    "    df_out = pd.DataFrame(arr).add_prefix(f\"{c}_svd\") #pca\n",
    "    assert df_input.shape[0] == df_out.shape[0]\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def long_title_tfidfvectorizer(df_input):\n",
    "    \"\"\"\n",
    "    long_titleのTfidf(text cleaningはしない)\n",
    "    \"\"\"\n",
    "    pipeline_ = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('svd', TruncatedSVD(n_components=50, n_iter=10, random_state=42)),\n",
    "    ])\n",
    "    c = \"long_title\"\n",
    "    pipeline_.fit(df_input[c][:len(train_data)].fillna(\"NULL\"))\n",
    "    arr = pipeline_.transform(df_input[c].fillna(\"NULL\"))\n",
    "    #arr = arr.toarray()\n",
    "    df_out = pd.DataFrame(arr).add_prefix(f\"{c}_svd\")\n",
    "    assert df_input.shape[0] == df_out.shape[0]\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def remove_pad_token(s):\n",
    "    \"\"\"\n",
    "    翻訳後のtextは頭に<pad> tokenが付いているので消す\n",
    "    \"\"\"\n",
    "    if s is np.nan:\n",
    "        return np.nan\n",
    "    \n",
    "    clean_s = re.sub(\"<pad>\", \"\", s)\n",
    "    if clean_s[0] == \" \":\n",
    "        clean_s = clean_s[1:]\n",
    "    return clean_s\n",
    "\n",
    "\n",
    "def transed_text_series(df_input, p):\n",
    "    \"\"\"\n",
    "    翻訳後textのcsvを読み込みdf_inputにmergeさせる\n",
    "    \"\"\"\n",
    "    _df = pd.read_csv(p)\n",
    "    if \"transed_long_title\" in p:\n",
    "        c = \"transed_long_title\"\n",
    "        \n",
    "    if \"transed_description\" in p:\n",
    "        c = \"transed_description\"\n",
    "        \n",
    "    _df[c] = _df[c].apply(lambda x:remove_pad_token(x))\n",
    "    text_series = pd.merge(df_input, _df, on=\"object_id\", how=\"left\")[c]\n",
    "    \n",
    "    return text_series\n",
    "\n",
    "\n",
    "\n",
    "def transed_long_title_tfidf(df_input):\n",
    "    \"\"\"\n",
    "    long_titleをDutch->Englishに翻訳したtextに対するTfidf\n",
    "    \"\"\"\n",
    "    l = []\n",
    "    for p in [\"transed_long_title\"]: \n",
    "        series = transed_text_series(df_input, f\"../feature/{p}.csv\")\n",
    "        series = series.apply(lambda x:texthero_preprocessing(x))\n",
    "        \n",
    "        pipeline_ = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('svd', TruncatedSVD(n_components=50, n_iter=10, random_state=42)),\n",
    "        ])\n",
    "        pipeline_.fit(series[:len(train_data)])\n",
    "        arr = pipeline_.transform(series)\n",
    "        df_out = pd.DataFrame(arr).add_prefix(f\"{p}_tfidf_svd\")\n",
    "        l.append(df_out)\n",
    "        \n",
    "    df_out = pd.concat(l, axis=1)\n",
    "    assert df_input.shape[0] == df_out.shape[0]\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "def texthero_preprocessing(s):\n",
    "    \"\"\"\n",
    "    textheroが手元ではinstallできなかったので愚直にcleaningを関数化したもの\n",
    "    \"\"\"\n",
    "    \n",
    "    if s is np.nan:\n",
    "        return \"NULL\"\n",
    "    s = s.lower()\n",
    "    pattern = r\"\\b\\d+\\b\"\n",
    "    s = re.sub(pattern, \"X\", s) #remove_logits\n",
    "    s = re.sub(rf\"([{string.punctuation}])+\", \" \", s)\n",
    "    nfkd_form = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join([char for char in nfkd_form if not unicodedata.combining(char)])\n",
    "    \n",
    "    custom_stopwords = nltk.corpus.stopwords.words('dutch') + nltk.corpus.stopwords.words('english')\n",
    "    # Set flag to allow verbose regexps\n",
    "    # Words with optional internal hyphens \n",
    "    # Any space\n",
    "    # Any symbol \n",
    "    pattern = r\"\"\"(?x)                          \n",
    "      \\w+(?:-\\w+)*                              \n",
    "      | \\s*                                     \n",
    "      | [][!\"#$%&'*+,-./:;<=>?@\\\\^():_`{|}~]   \n",
    "    \"\"\"\n",
    "    s = \"\".join(t if t not in custom_stopwords else \" \" for t in re.findall(pattern, s))\n",
    "    \n",
    "    s = \" \".join(re.sub(\"\\xa0\", \" \", s).split())\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def clean_tfidf_title_desc(df_input):\n",
    "    \"\"\"\n",
    "    titleとdescriptionをcleaningし結合．それに対してTfidf\n",
    "    TruncatedSVDのrandom_stateを指定し忘れており再現を妨げている\n",
    "    \"\"\"\n",
    "    text_series = df_input[\"title\"] + \" \" + df_input[\"description\"].fillna(\"NULL\")\n",
    "    clean_series = text_series.apply(lambda x:texthero_preprocessing(x))\n",
    "    \n",
    "    pipeline_ = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('svd', TruncatedSVD(n_components=64, n_iter=10, random_state=42)),\n",
    "    ])\n",
    "    pipeline_.fit(clean_series[:len(train_data)])\n",
    "    arr = pipeline_.transform(clean_series)\n",
    "    df_out = pd.DataFrame(arr).add_prefix(f\"title_desc_svd\") #pca\n",
    "    assert df_input.shape[0] == df_out.shape[0]\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lazy_text_feature(_df_input):\n",
    "    \"\"\"\n",
    "    文字長さ関係の特徴量\n",
    "    \"\"\"\n",
    "    df_input = _df_input.copy()\n",
    "    df_out = pd.DataFrame()\n",
    "    for c in [\"title\", \"description\", \"long_title\", \"more_title\", \"sub_title\"]:\n",
    "        df_input[c] = np.where(df_input[c].isnull(), \"\", df_input[c])\n",
    "        df_out[c + \"_len\"] = df_input[c].apply(lambda x:len(x))\n",
    "        df_out[\"clean_\" + c + \"_len\"] = df_input[c].apply(lambda x:texthero_preprocessing(x)).str.len()\n",
    "        \n",
    "        df_out[c + \"_len_percentage\"] = df_out[\"clean_\" + c + \"_len\"] / df_out[c + \"_len\"]\n",
    "        #df_out[c + \"_Nwords\"] = df_input[c].apply(lambda x:len(x.split()))\n",
    "        \n",
    "    df_out[\"long_div_title_len\"] = df_out[\"long_title_len\"] / df_out[\"title_len\"]\n",
    "    df_out[\"more_div_title_len\"] = df_out[\"more_title_len\"] / df_out[\"title_len\"]\n",
    "    \n",
    "    df_out[\"year_is_circa\"] = df_input[\"dating_presenting_date\"].str.contains(\"c\").astype(float)\n",
    "    assert df_input.shape[0] == df_out.shape[0]\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "def sentiment_feature(df_input):\n",
    "    \"\"\"\n",
    "    twitter sentimentのcsvを結合\n",
    "    \"\"\"\n",
    "    outputs_list = []\n",
    "    for name in [\"long_title\", \"principal_maker\", \"title\"]:\n",
    "        name = name + \"_tweet_sentiment\"\n",
    "        feat = pd.read_csv(f\"../feature/{name}.csv\")\n",
    "        cols = [c for c in feat.columns if c!=\"object_id\"]\n",
    "        tmp_out = pd.merge(df_input, feat, on=\"object_id\", how=\"left\")[cols]\n",
    "        tmp_out = tmp_out.add_prefix(f\"{name}_\")\n",
    "\n",
    "        softmax_arr = softmax(tmp_out.values, axis=1)\n",
    "        tmp_out = pd.concat([tmp_out,\n",
    "                            pd.DataFrame(softmax_arr, columns=cols).add_prefix(f\"{name}_SF_\")], axis=1)\n",
    "        outputs_list.append(tmp_out)\n",
    "        \n",
    "    for name in [\"long_title\", \"description\"]:\n",
    "        name = \"transed_\" + name + \"_tweet_sentiment\"\n",
    "        feat = pd.read_csv(f\"../feature/{name}.csv\")\n",
    "        cols = [c for c in feat.columns if c!=\"object_id\"]\n",
    "        tmp_out = pd.merge(df_input, feat, on=\"object_id\", how=\"left\")[cols]\n",
    "        tmp_out = tmp_out.add_prefix(f\"{name}_\")\n",
    "\n",
    "        softmax_arr = softmax(tmp_out.values, axis=1)\n",
    "        tmp_out = pd.concat([tmp_out,\n",
    "                            pd.DataFrame(softmax_arr, columns=cols).add_prefix(f\"{name}_SF_\")], axis=1)\n",
    "        outputs_list.append(tmp_out)\n",
    "        \n",
    "    df_out = pd.concat(outputs_list, axis=1)\n",
    "    assert df_input.shape[0] == df_out.shape[0]\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def more_title_diff_feature(df_input):\n",
    "    \"\"\"\n",
    "    more_titleとtitleの差分を取ってCountVector\n",
    "    \"\"\"\n",
    "    df_out = pd.DataFrame()\n",
    "    df_out[\"more_minus_title\"] = df_input[\"more_title\"].copy()\n",
    "    null_idx = df_input[\"more_title\"].isnull()\n",
    "    \n",
    "    df_out[\"more_minus_title\"][~null_idx] = df_input[[\"title\", \"more_title\"]].dropna().apply(lambda x:x[1].replace(x[0], ''), axis=1)                 \n",
    "    df_out = count_encoder(df_out, df_out.columns)\n",
    "    assert df_input.shape[0] == df_out.shape[0]\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def more_title_tfidf(df_input):\n",
    "    \"\"\"\n",
    "    more_titleのtfidf\n",
    "    TruncatedSVDのrandom_stateを指定し忘れており再現の妨げになっている\n",
    "    \"\"\"\n",
    "    df_out = pd.DataFrame()\n",
    "    tmp_df = df_input[[\"title\", \"more_title\"]].copy()\n",
    "    tmp_df[\"title\"] = tmp_df[\"title\"].apply(lambda x:texthero_preprocessing(x))\n",
    "    tmp_df[\"more_title\"] = tmp_df[\"more_title\"].fillna(\"\").apply(lambda x:texthero_preprocessing(x))\n",
    "    \n",
    "    more_minus_title = tmp_df[[\"title\", \"more_title\"]].dropna().apply(lambda x:x[1].replace(x[0], ''), axis=1)\n",
    "    pipeline_ = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(min_df=5, ngram_range=(1,3))),\n",
    "        ('svd', TruncatedSVD(n_components=12, n_iter=10, random_state=42)),\n",
    "    ])\n",
    "    pipeline_.fit(more_minus_title[:len(train_data)])\n",
    "    arr = pipeline_.transform(more_minus_title)\n",
    "    df_out = pd.DataFrame(arr).add_prefix(f\"more_title_svd\") #pca\n",
    "    \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SWEM用の単語ベクトル\n",
    "\"\"\"\n",
    "from gensim.models import word2vec, KeyedVectors\n",
    "import gensim.downloader as api\n",
    "model = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(s):\n",
    "    s = \" \".join(re.sub(\"\\xa0\", \" \", s).split())\n",
    "    return s\n",
    "\n",
    "\n",
    "def extract_word2vec_arr(df_input, cols):\n",
    "    \"\"\"\n",
    "    追加テーブル(material.csv等)に対してSWEM\n",
    "    \"\"\"\n",
    "    feat = []\n",
    "    for idx in df_input.index:\n",
    "        sentence_vec = []\n",
    "        for c in cols:\n",
    "            sentence = df_input.loc[idx, c]\n",
    "            if sentence is np.nan:\n",
    "                #feat.append([np.nan])\n",
    "                continue\n",
    "            \n",
    "            text_list = sentence.split(\",\")\n",
    "            for w in text_list:\n",
    "                w = remove_whitespace(w)\n",
    "                try:\n",
    "                    vec_arr = model[w]\n",
    "                    sentence_vec.append(vec_arr)\n",
    "                    continue\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                for _w in w.split(\" \"):\n",
    "                    _w = remove_whitespace(_w)\n",
    "                    try:\n",
    "                        vec_arr = model[_w]\n",
    "                        sentence_vec.append(vec_arr)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "            \n",
    "        if len(sentence_vec)==0:\n",
    "            sentence_vec = [np.ones(100)*np.nan]\n",
    "\n",
    "        sentence_vec = np.array(sentence_vec).mean(axis=0)\n",
    "        feat.append(sentence_vec)\n",
    "\n",
    "    feat_df = pd.DataFrame(feat)\n",
    "    return feat_df\n",
    "\n",
    "\n",
    "def pretrained_w2v_feature(df_input):\n",
    "    _df = merge_tables(df_input)\n",
    "    df_out = extract_word2vec_arr(_df, [\"tech\", \"material\"]).add_prefix(\"tech_mate_w2v_\")\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec, KeyedVectors\n",
    "from tqdm import tqdm\n",
    "def tables_word2vec_feature(df_input):\n",
    "    \"\"\"\n",
    "    Arai-san discussion w2v\n",
    "    \"\"\"\n",
    "    material = pd.read_csv(DATADIR + \"material.csv\")\n",
    "    technique = pd.read_csv(DATADIR + \"technique.csv\")\n",
    "    collection = pd.read_csv(DATADIR + \"object_collection.csv\")\n",
    "    person = pd.read_csv(DATADIR + \"historical_person.csv\")\n",
    "    place = pd.read_csv(DATADIR + \"production_place.csv\")\n",
    "\n",
    "    mat_col = pd.concat([material, collection], axis=0).reset_index(drop=True)\n",
    "    mat_tec = pd.concat([material, technique], axis=0).reset_index(drop=True)\n",
    "    col_tec = pd.concat([collection, technique], axis=0).reset_index(drop=True)\n",
    "    mat_col_tec = pd.concat([material, collection, technique], axis=0).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    model_size = {\n",
    "        \"material\": 10,\n",
    "        \"technique\": 8,\n",
    "        \"collection\": 3,\n",
    "        \"material_collection\": 20,\n",
    "        \"material_technique\": 20,\n",
    "        \"collection_technique\": 10,\n",
    "        \"material_collection_technique\": 25\n",
    "    }\n",
    "\n",
    "    n_iter = 100\n",
    "\n",
    "\n",
    "    w2v_dfs = []\n",
    "    for df, df_name in tqdm(zip(\n",
    "            [\n",
    "                material, collection, technique,\n",
    "                mat_col, mat_tec, col_tec, mat_col_tec\n",
    "            ], [\n",
    "                \"material\", \"collection\", \"technique\",\n",
    "                \"material_collection\",\n",
    "                \"material_technique\",\n",
    "                \"collection_technique\",\n",
    "                \"material_collection_technique\"\n",
    "            ])):\n",
    "        df_group = df.groupby(\"object_id\")[\"name\"].apply(list).reset_index()\n",
    "        # Word2Vecの学習\n",
    "        w2v_model = word2vec.Word2Vec(df_group[\"name\"].values.tolist(),\n",
    "                                      size=model_size[df_name],\n",
    "                                      min_count=1,\n",
    "                                      window=1,\n",
    "                                      iter=n_iter)\n",
    "\n",
    "        # 各文章ごとにそれぞれの単語をベクトル表現に直し、平均をとって文章ベクトルにする\n",
    "        sentence_vectors = df_group[\"name\"].apply(\n",
    "            lambda x: np.mean([w2v_model.wv[e] for e in x], axis=0))\n",
    "        sentence_vectors = np.vstack([x for x in sentence_vectors])\n",
    "        sentence_vector_df = pd.DataFrame(sentence_vectors,\n",
    "                                          columns=[f\"{df_name}_w2v_{i}\"\n",
    "                                                   for i in range(model_size[df_name])])\n",
    "        sentence_vector_df.index = df_group[\"object_id\"]\n",
    "        w2v_dfs.append(sentence_vector_df)\n",
    "        \n",
    "    feat_df = pd.concat(w2v_dfs, axis=1).reset_index().rename(columns={\"index\":\"object_id\"})\n",
    "    cols = [c for c in feat_df.columns if c!=\"object_id\"]\n",
    "    df_out = pd.merge(df_input, feat_df, on=\"object_id\", how=\"left\")\n",
    "    return df_out[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# maintable_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_index(s, word):\n",
    "    try:\n",
    "        return s[int(s.index(word)+1)]\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "def cm2mm(s):\n",
    "    try:\n",
    "        num = float(re.sub(\"[a-zA-Z]\", \"\", s))\n",
    "        if len(str(num))==0:\n",
    "            return np.nan\n",
    "        elif \"cm\" in s:\n",
    "            return num * 10\n",
    "        else:# \"mm\" in s:\n",
    "            return num\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "    \n",
    "def subtitle_feature(df_input):\n",
    "    \"\"\"\n",
    "    sub_titleから数値抽出\n",
    "    \"\"\"\n",
    "    df_out = pd.DataFrame()\n",
    "    _df = df_input[[\"sub_title\"]].copy()\n",
    "    _df[\"sub_title\"] = np.where(_df[\"sub_title\"].isnull(), \"NULL\", _df[\"sub_title\"])\n",
    "    _df[\"sub_title\"] = _df[\"sub_title\"].apply(lambda x:[w for w in x.split(\" \") if w !=\"×\"])\n",
    "    \n",
    "    for w in [\"h\", \"w\", \"t\", \"d\", \"l\"]:\n",
    "        df_out[w] = _df[\"sub_title\"].apply(lambda x:search_index(x, w))\n",
    "        df_out[w] = df_out[w].apply(lambda x:cm2mm(x))\n",
    "        \n",
    "    df_out[\"num_unit\"] = df_input[\"sub_title\"].str.split(\" × \", expand=True).isnull().sum(axis=1)\n",
    "    \n",
    "    #df_out[\"weight\"] = df_input[\"sub_title\"].apply(lambda x:\"\".join([w for w in x.split(\" \") if \"g\" in w]))\n",
    "    df_out[\"weight\"] = _df[\"sub_title\"].apply(lambda x:\"\".join([w for w in x if \"g\" in w]))\n",
    "    df_out[\"weight\"] = df_out[\"weight\"].apply(lambda x:re.sub(\"[^0-9]\", \"\", x))\n",
    "    df_out[\"weight\"] = np.where(df_out[\"weight\"].apply(lambda x:len(x))==0, np.nan, df_out[\"weight\"]).astype(float)\n",
    "    \n",
    "    df_out[\"size\"] = df_out[\"h\"] * df_out[\"w\"]\n",
    "    df_out[\"volume\"] = df_out[\"size\"] * df_out[\"d\"]\n",
    "    \n",
    "    assert df_input.shape[0] == df_out.shape[0]\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jumble_feature(df_input):\n",
    "    \"\"\"\n",
    "    main table周りで思いついた特徴の寄せ集め\n",
    "    \"\"\"\n",
    "    df_out = pd.DataFrame()\n",
    "    \n",
    "    df_out[\"acquisition_year\"] = pd.to_datetime(df_input[\"acquisition_date\"]).dt.year\n",
    "    df_out[\"dating_year_diff\"] = df_input[\"dating_year_late\"] - df_input[\"dating_year_early\"]\n",
    "    \n",
    "    df_out[\"principal_maker_mismatch\"] = (df_input[\"principal_maker\"] != df_input[\"principal_or_first_maker\"]).astype(int)\n",
    "    df_out[\"c_in_dating_presenting_date\"] = df_input[\"dating_presenting_date\"].str.contains(\"c\").astype(float)\n",
    "    \n",
    "    _df = df_input.groupby(\"principal_maker\")[\"dating_sorting_date\"].agg([\"size\", \"nunique\"])\n",
    "    dic = (_df[\"nunique\"] / _df[\"size\"]).to_dict()\n",
    "    df_out[\"principal_maker_info\"] = df_input[\"principal_maker\"].map(dic)\n",
    "    \n",
    "    df_out[\"acquisition_year_from_made\"] = pd.to_datetime(df_input[\"acquisition_date\"]).dt.year - df_input[\"dating_year_early\"]\n",
    "    \n",
    "    assert df_input.shape[0] == df_out.shape[0]\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def groupby_feature(df_input):\n",
    "    tables = merge_tables(all_df)\n",
    "    for name in [\"color\", \"palette\"]:\n",
    "        color_df = pd.read_csv(DATADIR + f\"{name}.csv\")\n",
    "        color_ids = color_df[\"object_id\"].unique()\n",
    "        tables[name] = np.where(all_df[\"object_id\"].isin(color_ids),\n",
    "                                1,\n",
    "                                np.nan)\n",
    "    \n",
    "    tables = pd.concat([tables, \n",
    "                        subtitle_feature(all_df)[[\"h\", \"w\", \"d\", \"l\", \"weight\"]]], axis=1)\n",
    "    \n",
    "    cols = tables.columns[tables.isnull().astype(int).sum(axis=0) > 10]\n",
    "    cols = [c for c in cols if c not in [\"dating_sorting_date\", \"dating_year_early\"]]\n",
    "    null_df = tables[cols].isnull().astype(int)\n",
    "    \n",
    "    gmm = GaussianMixture(n_components=10, random_state=0)\n",
    "    arr = gmm.fit_predict(null_df)\n",
    "    null_group = pd.Series(arr, name=\"null_group\")\n",
    "    \n",
    "    tmp_df = pd.concat([all_df, df_input, null_group], axis=1)\n",
    "    \n",
    "    df_out = pd.DataFrame()\n",
    "    for key in [\"principal_maker\", \"acquisition_method\", \"null_group\", \"title\"]:\n",
    "        for val in [\"dating_sorting_date\", \"h\", \"RGBsum_std\", \"RGBsum_min_prod_ratio\", \"random_resnet0\", \"min_ratio\"]:\n",
    "            dic = tmp_df.groupby(key)[val].mean().to_dict()\n",
    "            df_out[key+\"_GPB_\"+val] = tmp_df[val] - tmp_df[key].map(dic)\n",
    "    \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T11:47:33.586383Z",
     "iopub.status.busy": "2020-12-01T11:47:33.585428Z",
     "iopub.status.idle": "2020-12-01T11:47:33.589064Z",
     "shell.execute_reply": "2020-12-01T11:47:33.588249Z"
    },
    "papermill": {
     "duration": 0.037743,
     "end_time": "2020-12-01T11:47:33.589212",
     "exception": false,
     "start_time": "2020-12-01T11:47:33.551469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def raw_data(df_input):\n",
    "    cols = [\"dating_period\", \"dating_year_early\", \"dating_year_late\"]\n",
    "    df_out = df_input[cols].copy()\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def make_feature(df_input):\n",
    "    functions = [\n",
    "        raw_data,\n",
    "        count_feature,\n",
    "        jumble_feature,\n",
    "        lazy_text_feature,\n",
    "        \n",
    "        lazy_ratio_agg,\n",
    "        color_agg_feature,\n",
    "        color_palette_agg,\n",
    "        palette_feature,\n",
    "        color_feature,\n",
    "        \n",
    "        ExG_agg_feature,\n",
    "        color_ExG_agg_feature,\n",
    "        \n",
    "        \n",
    "        subtitle_feature,\n",
    "        lang_detect,\n",
    "        maker_countvectorizer,\n",
    "        description_tfidfvectorizer,\n",
    "        long_title_tfidfvectorizer,\n",
    "        clean_tfidf_title_desc,\n",
    "        \n",
    "        transed_long_title_tfidf,\n",
    "        bert_description,\n",
    "        sentiment_feature,\n",
    "        \n",
    "        #palette_HSV, #時短用, #あってもなくてもみたいなところある\n",
    "        \n",
    "        tables_onehot,\n",
    "        resnet50_feature,\n",
    "        \n",
    "        \n",
    "        null_features,\n",
    "        more_title_diff_feature,\n",
    "        more_title_tfidf, #これはいらんかも\n",
    "        lazy_merge_tables_feature,\n",
    "        \n",
    "        pretrained_w2v_feature,\n",
    "    ]\n",
    "    features = [f(df_input) for f in tqdm(functions)]\n",
    "    features = pd.concat(features, axis=1)\n",
    "    \n",
    "    features = pd.concat([features, groupby_feature(features)], axis=1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2 \n",
    "    dfs = []\n",
    "    for col in df.columns: #columns毎に処理\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics: #numericsのデータ型の範囲内のときに処理を実行. データの最大最小値を元にデータ型を効率的なものに変更\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    dfs.append(df[col].astype(np.int8))\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    dfs.append(df[col].astype(np.int16))\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    dfs.append(df[col].astype(np.int32))\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    dfs.append(df[col].astype(np.int64) ) \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    dfs.append(df[col].astype(np.float16))\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    dfs.append(df[col].astype(np.float32))\n",
    "                else:\n",
    "                    dfs.append(df[col].astype(np.float64))\n",
    "        else:\n",
    "            dfs.append(df[col])\n",
    "    \n",
    "    df_out = pd.concat(dfs, axis=1)\n",
    "    if verbose:\n",
    "        end_mem = df_out.memory_usage().sum() / 1024**2\n",
    "        num_reduction = str(100 * (start_mem - end_mem) / start_mem)\n",
    "        print(f'Mem. usage decreased to {str(end_mem)[:3]}Mb:  {num_reduction[:2]}% reduction')\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 12/27 [01:05<00:44,  2.97s/it]Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      " 81%|████████▏ | 22/27 [01:53<00:18,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [02:27<00:00,  5.44s/it]\n"
     ]
    }
   ],
   "source": [
    "features = make_feature(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 39.Mb:  75% reduction\n"
     ]
    }
   ],
   "source": [
    "features = reduce_mem_usage(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24034, 865)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "train = features[:len(train_data)].reset_index(drop=True)\n",
    "test = features[len(train_data):].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T11:47:33.644525Z",
     "iopub.status.busy": "2020-12-01T11:47:33.643573Z",
     "iopub.status.idle": "2020-12-01T11:47:33.646888Z",
     "shell.execute_reply": "2020-12-01T11:47:33.646223Z"
    },
    "papermill": {
     "duration": 0.0354,
     "end_time": "2020-12-01T11:47:33.647087",
     "exception": false,
     "start_time": "2020-12-01T11:47:33.611687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_rmse(y, y_oof):\n",
    "    score = ((y.values.reshape(1,-1) - y_oof)**2).mean()**0.5\n",
    "    return score\n",
    "\n",
    "params = {\n",
    "    \"boosting_type\":'gbdt',\n",
    "    \"num_leaves\":34,  #18, #34\n",
    "    \"max_depth\":-1,\n",
    "    \"learning_rate\":0.01, #0.1\n",
    "    \"n_estimators\":20000,\n",
    "    \n",
    "    \"objective\":\"regression\",\n",
    "    \n",
    "    \"metric\":\"rmse\", #rmse mae\n",
    "    \"force_col_wise\":True,\n",
    "    \"bin_construct_sample_cnt\":2000,\n",
    "    \n",
    "    \"bagging_freq\": 3,  #3\n",
    "    \"subsample\":0.7,#0.7\n",
    "    \n",
    "    \"colsample_bytree\":0.5,\n",
    "    \"reg_alpha\":.7, \n",
    "    \"reg_lambda\":.1, #l2\n",
    "    \"random_state\":42,\n",
    "    \"n_jobs\":-1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T11:47:33.712402Z",
     "iopub.status.busy": "2020-12-01T11:47:33.711303Z",
     "iopub.status.idle": "2020-12-01T11:47:33.714160Z",
     "shell.execute_reply": "2020-12-01T11:47:33.713519Z"
    },
    "papermill": {
     "duration": 0.043635,
     "end_time": "2020-12-01T11:47:33.714296",
     "exception": false,
     "start_time": "2020-12-01T11:47:33.670661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_lgbm(_X, y, _test, splits, params, y_max, feature_selection=False):\n",
    "    X = _X.copy()\n",
    "    test = _test.copy()\n",
    "    if feature_selection:\n",
    "        feature_df = pd.read_csv(OUTPUT_DIR + \"feature_importances_lgbm_False.csv\")\n",
    "        feature_df[\"average\"].hist()\n",
    "        plt.show()\n",
    "        feature_arr = feature_df[feature_df[\"average\"]>150][\"feature\"]\n",
    "        print(_X.columns, \"->\", feature_arr.shape)\n",
    "        X = _X[feature_arr]\n",
    "        test = _test[feature_arr]\n",
    "        \n",
    "    X.to_pickle(OUTPUT_DIR + \"features.pkl\")\n",
    "    feature_importances = pd.DataFrame(data=X.columns, columns=[\"feature\"])\n",
    "    \n",
    "    y_oof = np.zeros(X.shape[0])\n",
    "    y_pred = np.zeros(test.shape[0])\n",
    "    scores = []\n",
    "    for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "        print(\"fold\", i)\n",
    "        X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n",
    "        y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n",
    "\n",
    "        idx_use = y_train < y_max\n",
    "        y_train = np.where(idx_use, y_train, y_max)\n",
    "\n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        lgb_valid = lgb.Dataset(X_valid, y_valid)\n",
    "        lgb_clf = lgb.train(params, lgb_train,\n",
    "                            valid_names=[\"train\", \"valid\"], \n",
    "                            valid_sets=[lgb_train, lgb_valid], \n",
    "                            early_stopping_rounds=500, #100, 0311\n",
    "                            verbose_eval=500,\n",
    "                           )\n",
    "        \n",
    "        oof = lgb_clf.predict(X_valid)\n",
    "        y_oof[valid_idx] = oof\n",
    "        \n",
    "        y_pred_per_fold = lgb_clf.predict(test)\n",
    "        y_pred += y_pred_per_fold / N_SPLIT\n",
    "        \n",
    "        feature_importances[i] = lgb_clf.feature_importance()\n",
    "        \n",
    "        score = calc_rmse(y_valid, oof)\n",
    "        print(f\"score:{score}\")\n",
    "        scores.append(score)\n",
    "        pickle.dump(lgb_clf, open(OUTPUT_DIR + f'lgbm_model_fold{i}_{feature_selection}.pkl', 'wb'))\n",
    "        \n",
    "        sample = pd.read_csv(DATADIR + \"atmacup10__sample_submission.csv\")\n",
    "        sample[\"likes\"] = y_pred_per_fold\n",
    "        sample.to_csv(OUTPUT_DIR + f\"submission_lgbm{i}_{feature_selection}_{score}.csv\", index=False)\n",
    "        \n",
    "\n",
    "    score = calc_rmse(y, y_oof)\n",
    "    print(scores)\n",
    "    print(f\"score:{score}\")\n",
    "    scores.append(score)\n",
    "    scores = np.array(scores)\n",
    "    \n",
    "    feature_importances['average'] = feature_importances.mean(axis=1)\n",
    "    feature_importances = feature_importances.sort_values(\"average\", ascending=False)\n",
    "    \n",
    "    y_oof = np.expm1(y_oof)\n",
    "    y_oof = np.where(y_oof < 0, 0, y_oof)\n",
    "    y_pred = np.expm1(y_pred)\n",
    "    y_pred = np.where(y_pred < 0, 0, y_pred)\n",
    "        \n",
    "    sample = pd.read_csv(DATADIR + \"atmacup10__sample_submission.csv\")\n",
    "    print(y_pred.shape, sample.shape)\n",
    "    sample[\"likes\"] = y_pred\n",
    "    sample.to_csv(OUTPUT_DIR + f\"submission_lgbm_{feature_selection}.csv\", index=False)\n",
    "    \n",
    "    pd.concat([pd.Series(y_oof), pd.Series(y_pred)], axis=0).reset_index(drop=True).to_csv(OUTPUT_DIR + f\"oof_and_pred_lgbm_{feature_selection}.csv\", index=False)\n",
    "    feature_importances.to_csv(OUTPUT_DIR + f\"feature_importances_lgbm_{feature_selection}.csv\",index=False)\n",
    "    return y_oof, y_pred, scores, feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_class_weight(target):\n",
    "    weight = np.where(target==0, \n",
    "                      np.ones(target.shape[0])*((target==1).sum() / (target==0).sum()), \n",
    "                      np.ones(target.shape[0]))\n",
    "    return weight\n",
    "\n",
    "\n",
    "def run_lgbm_pseudo(_X, _y, _test, splits, params, y_max, feature_selection=False):\n",
    "    X = _X.copy()\n",
    "    test = _test.copy()\n",
    "    y = _y.copy()\n",
    "    X.to_pickle(OUTPUT_DIR + \"features_pseudo.pkl\")\n",
    "    feature_importances = pd.DataFrame(data=X.columns, columns=[\"feature\"])\n",
    "    \n",
    "    y_oof = np.zeros(X.shape[0])\n",
    "    y_pred = np.zeros(test.shape[0])\n",
    "    scores = []\n",
    "    \n",
    "    weight_series = pd.concat([pd.Series(np.ones(len(train_data))),\n",
    "                           pd.Series(np.ones(len(test_data)) * 0.5)], axis=0).reset_index(drop=True)\n",
    "    weight_series = weight_series.rename(\"weight\")\n",
    "    \n",
    "    _params = {\n",
    "    \"boosting_type\":'gbdt',\n",
    "    \"num_leaves\":34,  #18, #34\n",
    "    \"max_depth\":-1,\n",
    "    \"learning_rate\":0.1, #0.1\n",
    "    \"n_estimators\":20000,\n",
    "    \n",
    "    \"objective\":\"regression\",\n",
    "    \n",
    "    \"metric\":\"rmse\", #rmse mae\n",
    "    \"force_col_wise\":True,\n",
    "    \"bin_construct_sample_cnt\":2000,\n",
    "    \n",
    "    \"bagging_freq\": 3,  #3\n",
    "    \"subsample\":0.7,#0.7\n",
    "    \n",
    "    \"colsample_bytree\":0.5, #0.7 \n",
    "    \"reg_alpha\":.7, #1. #l1 (.7, .1)\n",
    "    \"reg_lambda\":.1, #l2\n",
    "    \"random_state\":42,\n",
    "    \"n_jobs\":-1,\n",
    "    \"verbose\":-1,\n",
    "    }\n",
    "    \n",
    "    \n",
    "    for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "        print(\"fold\", i)\n",
    "        print(X.shape, test.shape)\n",
    "        X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n",
    "        y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n",
    "\n",
    "        idx_use = y_train < y_max\n",
    "        y_train = np.where(idx_use, y_train, y_max)\n",
    "        \n",
    "        weight_train = weight_series.loc[train_idx]\n",
    "        weight_test = weight_series.loc[valid_idx]\n",
    "\n",
    "        lgb_train = lgb.Dataset(X_train, y_train, weight=weight_train)\n",
    "        lgb_valid = lgb.Dataset(X_valid, y_valid, weight=weight_test)\n",
    "                               \n",
    "        lgb_clf = lgb.train(_params, lgb_train,\n",
    "                            valid_names=[\"train\", \"valid\"], \n",
    "                            valid_sets=[lgb_train, lgb_valid], \n",
    "                            early_stopping_rounds=500, #100, 0311\n",
    "                            verbose_eval=500,\n",
    "                           )\n",
    "        \n",
    "        oof = lgb_clf.predict(X_valid)\n",
    "        y_oof[valid_idx] = oof\n",
    "        \n",
    "        y_pred_per_fold = lgb_clf.predict(test)\n",
    "        y_pred += y_pred_per_fold / N_SPLIT\n",
    "        \n",
    "        feature_importances[i] = lgb_clf.feature_importance()\n",
    "        \n",
    "        score = calc_rmse(y_valid, oof) #log_loss\n",
    "        print(f\"score:{score}\")\n",
    "        scores.append(score)\n",
    "        pickle.dump(lgb_clf, open(OUTPUT_DIR + f'lgbm_model_fold{i}_{feature_selection}_pseudo.pkl', 'wb'))\n",
    "        \n",
    "        sample = pd.read_csv(DATADIR + \"atmacup10__sample_submission.csv\")\n",
    "        sample[\"likes\"] = y_pred_per_fold\n",
    "        sample.to_csv(OUTPUT_DIR + f\"submission_lgbm{i}_{feature_selection}_{score}_pseudo.csv\", index=False)\n",
    "        \n",
    "\n",
    "    score = calc_rmse(y, y_oof)\n",
    "    print(scores)\n",
    "    print(f\"score:{score}\")\n",
    "    scores.append(score)\n",
    "    scores = np.array(scores)\n",
    "    \n",
    "    feature_importances['average'] = feature_importances.mean(axis=1)\n",
    "    feature_importances = feature_importances.sort_values(\"average\", ascending=False)\n",
    "    \n",
    "    y_oof = np.expm1(y_oof)\n",
    "    y_oof = np.where(y_oof < 0, 0, y_oof)\n",
    "    y_pred = np.expm1(y_pred)\n",
    "    y_pred = np.where(y_pred < 0, 0, y_pred)\n",
    "        \n",
    "    sample = pd.read_csv(DATADIR + \"atmacup10__sample_submission.csv\")\n",
    "    print(y_pred.shape, sample.shape)\n",
    "    sample[\"likes\"] = y_pred\n",
    "    sample.to_csv(OUTPUT_DIR + f\"submission_lgbm_{feature_selection}_pseudo.csv\", index=False)\n",
    "    \n",
    "    pd.concat([pd.Series(y_oof), pd.Series(y_pred)], axis=0).reset_index(drop=True).to_csv(OUTPUT_DIR + f\"oof_and_pred_lgbm_{feature_selection}_pseudo.csv\", index=False)\n",
    "    feature_importances.to_csv(OUTPUT_DIR + f\"feature_importances_lgbm_{feature_selection}_pseudo.csv\",index=False)\n",
    "    return y_oof, y_pred, scores, feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T11:47:33.772910Z",
     "iopub.status.busy": "2020-12-01T11:47:33.771725Z",
     "iopub.status.idle": "2020-12-01T11:47:33.775487Z",
     "shell.execute_reply": "2020-12-01T11:47:33.774702Z"
    },
    "papermill": {
     "duration": 0.036531,
     "end_time": "2020-12-01T11:47:33.775629",
     "exception": false,
     "start_time": "2020-12-01T11:47:33.739098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_kfolds(df_input, N_SPLIT, SEED):\n",
    "    folds = KFold(n_splits=N_SPLIT, random_state=SEED, shuffle=True)\n",
    "    splits = folds.split(df_input)\n",
    "    return splits\n",
    "\n",
    "\n",
    "def make_stratifirdkfolds(_y, N_SPLIT, SEED):\n",
    "    folds = StratifiedKFold(n_splits=N_SPLIT, random_state=SEED, shuffle=True)\n",
    "    splits = folds.split(_y, pd.cut(_y, 8, labels=False))\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../input/train.csv\")\n",
    "y = train_data[\"likes\"].copy()\n",
    "y = np.log1p(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2020-12-01T11:47:33.833120Z",
     "iopub.status.busy": "2020-12-01T11:47:33.832212Z",
     "iopub.status.idle": "2020-12-01T11:48:27.675786Z",
     "shell.execute_reply": "2020-12-01T11:48:27.676961Z"
    },
    "papermill": {
     "duration": 53.877839,
     "end_time": "2020-12-01T11:48:27.677186",
     "exception": false,
     "start_time": "2020-12-01T11:47:33.799347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ueda/.pyenv/versions/3.6.5/lib/python3.6/site-packages/lightgbm/engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 155899\n",
      "[LightGBM] [Info] Number of data points in the train set: 9620, number of used features: 864\n",
      "[LightGBM] [Info] Start training from score 1.654172\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttrain's rmse: 0.771079\tvalid's rmse: 1.02143\n",
      "[1000]\ttrain's rmse: 0.604058\tvalid's rmse: 0.999074\n",
      "[1500]\ttrain's rmse: 0.492135\tvalid's rmse: 0.989559\n",
      "[2000]\ttrain's rmse: 0.407586\tvalid's rmse: 0.98425\n",
      "[2500]\ttrain's rmse: 0.34135\tvalid's rmse: 0.981857\n",
      "[3000]\ttrain's rmse: 0.288485\tvalid's rmse: 0.980461\n",
      "[3500]\ttrain's rmse: 0.24575\tvalid's rmse: 0.97949\n",
      "[4000]\ttrain's rmse: 0.210453\tvalid's rmse: 0.979082\n",
      "[4500]\ttrain's rmse: 0.181346\tvalid's rmse: 0.978444\n",
      "[5000]\ttrain's rmse: 0.157311\tvalid's rmse: 0.9783\n",
      "[5500]\ttrain's rmse: 0.137089\tvalid's rmse: 0.978051\n",
      "[6000]\ttrain's rmse: 0.120099\tvalid's rmse: 0.977854\n",
      "[6500]\ttrain's rmse: 0.105705\tvalid's rmse: 0.977642\n",
      "[7000]\ttrain's rmse: 0.0934314\tvalid's rmse: 0.977707\n",
      "Early stopping, best iteration is:\n",
      "[6794]\ttrain's rmse: 0.0982102\tvalid's rmse: 0.977544\n",
      "score:0.9775440757463747\n",
      "fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ueda/.pyenv/versions/3.6.5/lib/python3.6/site-packages/lightgbm/engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 156293\n",
      "[LightGBM] [Info] Number of data points in the train set: 9621, number of used features: 865\n",
      "[LightGBM] [Info] Start training from score 1.655197\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttrain's rmse: 0.775598\tvalid's rmse: 1.00806\n",
      "[1000]\ttrain's rmse: 0.606262\tvalid's rmse: 0.981564\n",
      "[1500]\ttrain's rmse: 0.49229\tvalid's rmse: 0.972541\n",
      "[2000]\ttrain's rmse: 0.407564\tvalid's rmse: 0.967993\n",
      "[2500]\ttrain's rmse: 0.342115\tvalid's rmse: 0.965574\n",
      "[3000]\ttrain's rmse: 0.289206\tvalid's rmse: 0.963747\n",
      "[3500]\ttrain's rmse: 0.246418\tvalid's rmse: 0.963113\n",
      "[4000]\ttrain's rmse: 0.211525\tvalid's rmse: 0.96209\n",
      "[4500]\ttrain's rmse: 0.182365\tvalid's rmse: 0.961723\n",
      "[5000]\ttrain's rmse: 0.158327\tvalid's rmse: 0.961445\n",
      "[5500]\ttrain's rmse: 0.138114\tvalid's rmse: 0.961293\n",
      "Early stopping, best iteration is:\n",
      "[5274]\ttrain's rmse: 0.146846\tvalid's rmse: 0.961192\n",
      "score:0.9611923684520962\n",
      "fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ueda/.pyenv/versions/3.6.5/lib/python3.6/site-packages/lightgbm/engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 155831\n",
      "[LightGBM] [Info] Number of data points in the train set: 9621, number of used features: 865\n",
      "[LightGBM] [Info] Start training from score 1.654526\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttrain's rmse: 0.776183\tvalid's rmse: 0.993414\n",
      "[1000]\ttrain's rmse: 0.606775\tvalid's rmse: 0.971232\n",
      "[1500]\ttrain's rmse: 0.493659\tvalid's rmse: 0.962977\n",
      "[2000]\ttrain's rmse: 0.40885\tvalid's rmse: 0.959549\n",
      "[2500]\ttrain's rmse: 0.342834\tvalid's rmse: 0.957967\n",
      "[3000]\ttrain's rmse: 0.289701\tvalid's rmse: 0.957499\n",
      "[3500]\ttrain's rmse: 0.246871\tvalid's rmse: 0.956934\n",
      "[4000]\ttrain's rmse: 0.211728\tvalid's rmse: 0.956263\n",
      "[4500]\ttrain's rmse: 0.182739\tvalid's rmse: 0.956328\n",
      "Early stopping, best iteration is:\n",
      "[4288]\ttrain's rmse: 0.194256\tvalid's rmse: 0.956059\n",
      "score:0.9560592254427311\n",
      "fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ueda/.pyenv/versions/3.6.5/lib/python3.6/site-packages/lightgbm/engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 156352\n",
      "[LightGBM] [Info] Number of data points in the train set: 9621, number of used features: 864\n",
      "[LightGBM] [Info] Start training from score 1.655344\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttrain's rmse: 0.775994\tvalid's rmse: 0.998373\n",
      "[1000]\ttrain's rmse: 0.606026\tvalid's rmse: 0.975948\n",
      "[1500]\ttrain's rmse: 0.492519\tvalid's rmse: 0.967056\n",
      "[2000]\ttrain's rmse: 0.408113\tvalid's rmse: 0.963188\n",
      "[2500]\ttrain's rmse: 0.342137\tvalid's rmse: 0.960951\n",
      "[3000]\ttrain's rmse: 0.289307\tvalid's rmse: 0.959508\n",
      "[3500]\ttrain's rmse: 0.246546\tvalid's rmse: 0.958349\n",
      "[4000]\ttrain's rmse: 0.211615\tvalid's rmse: 0.95759\n",
      "[4500]\ttrain's rmse: 0.182615\tvalid's rmse: 0.957469\n",
      "[5000]\ttrain's rmse: 0.15852\tvalid's rmse: 0.957152\n",
      "[5500]\ttrain's rmse: 0.138297\tvalid's rmse: 0.956848\n",
      "[6000]\ttrain's rmse: 0.121386\tvalid's rmse: 0.956801\n",
      "[6500]\ttrain's rmse: 0.107009\tvalid's rmse: 0.956612\n",
      "[7000]\ttrain's rmse: 0.0946986\tvalid's rmse: 0.956598\n",
      "Early stopping, best iteration is:\n",
      "[6764]\ttrain's rmse: 0.100255\tvalid's rmse: 0.956509\n",
      "score:0.9565088176848886\n",
      "fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ueda/.pyenv/versions/3.6.5/lib/python3.6/site-packages/lightgbm/engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 155959\n",
      "[LightGBM] [Info] Number of data points in the train set: 9621, number of used features: 864\n",
      "[LightGBM] [Info] Start training from score 1.655451\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttrain's rmse: 0.778322\tvalid's rmse: 0.997716\n",
      "[1000]\ttrain's rmse: 0.608078\tvalid's rmse: 0.969847\n",
      "[1500]\ttrain's rmse: 0.494259\tvalid's rmse: 0.958981\n",
      "[2000]\ttrain's rmse: 0.409535\tvalid's rmse: 0.953631\n",
      "[2500]\ttrain's rmse: 0.343118\tvalid's rmse: 0.951094\n",
      "[3000]\ttrain's rmse: 0.290325\tvalid's rmse: 0.949989\n",
      "[3500]\ttrain's rmse: 0.247419\tvalid's rmse: 0.948781\n",
      "[4000]\ttrain's rmse: 0.211879\tvalid's rmse: 0.948186\n",
      "[4500]\ttrain's rmse: 0.182671\tvalid's rmse: 0.948132\n",
      "[5000]\ttrain's rmse: 0.158415\tvalid's rmse: 0.947827\n",
      "[5500]\ttrain's rmse: 0.138187\tvalid's rmse: 0.947635\n",
      "[6000]\ttrain's rmse: 0.121036\tvalid's rmse: 0.947421\n",
      "[6500]\ttrain's rmse: 0.10661\tvalid's rmse: 0.947235\n",
      "Early stopping, best iteration is:\n",
      "[6492]\ttrain's rmse: 0.106829\tvalid's rmse: 0.947216\n",
      "score:0.9472157266924734\n",
      "[0.9775440757463747, 0.9611923684520962, 0.9560592254427311, 0.9565088176848886, 0.9472157266924734]\n",
      "score:0.9597576267461986\n",
      "(12008,) (12008, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "N_SPLIT = 5\n",
    "target_col = \"likes\"\n",
    "\n",
    "_train = train.copy()\n",
    "_test = test.copy()\n",
    "\n",
    "_y = y.copy()\n",
    "\n",
    "assert _train.shape[0]==train.shape[0]\n",
    "assert _test.shape[0]==test.shape[0]\n",
    "\n",
    "splits = make_stratifirdkfolds(_y, N_SPLIT, SEED)\n",
    "y_oof, y_pred, scores, feature_importances = run_lgbm(_train, _y, _test, splits, params, 11, feature_selection=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nN_SPLIT = 5\\ntarget_col = \"likes\"\\n\\n_train = pd.concat([train.copy(), test.copy()], axis=0).reset_index(drop=True)\\n_test = test.copy()\\n_y = y.copy()\\n_y = pd.concat([_y, np.log1p(pd.read_csv(\"../output/start16/submission_lgbm_False.csv\")[\"likes\"])], axis=0).reset_index(drop=True)\\n\\nassert _train.shape[0]==(train.shape[0] + test.shape[0])\\nassert _test.shape[0]==test.shape[0]\\n\\nprint(_train.shape, _y.shape)\\n\\nsplits = make_stratifirdkfolds(_y, N_SPLIT, SEED)\\ny_oof, y_pred, scores, feature_importances = run_lgbm_pseudo(_train, _y, _test, splits, params, 11, feature_selection=False)\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "N_SPLIT = 5\n",
    "target_col = \"likes\"\n",
    "\n",
    "_train = pd.concat([train.copy(), test.copy()], axis=0).reset_index(drop=True)\n",
    "_test = test.copy()\n",
    "_y = y.copy()\n",
    "_y = pd.concat([_y, np.log1p(pd.read_csv(\"../output/start16/submission_lgbm_False.csv\")[\"likes\"])], axis=0).reset_index(drop=True)\n",
    "\n",
    "assert _train.shape[0]==(train.shape[0] + test.shape[0])\n",
    "assert _test.shape[0]==test.shape[0]\n",
    "\n",
    "print(_train.shape, _y.shape)\n",
    "\n",
    "splits = make_stratifirdkfolds(_y, N_SPLIT, SEED)\n",
    "y_oof, y_pred, scores, feature_importances = run_lgbm_pseudo(_train, _y, _test, splits, params, 11, feature_selection=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparams2 = params.copy()\\nparams2[\"objective\"] = \"regression\"\\nsplits = make_stratifirdkfolds(train_data, N_SPLIT, SEED)\\ny_oof, y_pred, scores, feature_importances = run_lgbm(train, y, test, splits, params2)\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainでfit, testでpredict\n",
    "# [0.9937712180369499, 0.9879254590058039, 0.9878856826781501, 0.9875887449174304, 0.986000326337547]\n",
    "# score:0.988638303123017\n",
    "\n",
    "# count encoderの追加とか, 変なlabel encoderの修正\n",
    "# [0.99512793349549, 0.9760252704277805, 0.9839311150065218, 0.982252494283172, 0.9875356866796321]\n",
    "# score:0.9849954729872427\n",
    "\n",
    "## onehot tables\n",
    "# [1.0045167537759587, 0.9772717821971039, 0.979420767098949, 0.9703726546630224, 0.9735351838633651]\n",
    "# score:0.9811006329513632\n",
    "\n",
    "# LGBM upgrade, 不適切なtableのcount encoding削除\n",
    "# [0.9930255688202387, 0.9753716568338878, 0.9802319472423626, 0.974020652037711, 0.9700600237968715]\n",
    "# score:0.9785753773934237\n",
    "\n",
    "# colorでExGとか, some_feature()とか,\n",
    "# principal makerでcount encoder ngram(1,1)大事, principal_firstに変えた途端CVは微改善でLB大幅down\n",
    "# [0.9876207284842683, 0.9749697685627453, 0.9663708610364038, 0.9683445892252829, 0.9678386797339881]\n",
    "# score:0.9730619935566562\n",
    "\n",
    "# null feature, sentimentとか_len_percentageとか\n",
    "# [0.9834260348382874, 0.9701033138631951, 0.9694680407449775, 0.9612780741465334, 0.9572818995892636]\n",
    "#score:0.9683544691954975\n",
    "\n",
    "# bagging 効きました\n",
    "# [0.9800986986278741, 0.9620824474869699, 0.9583931837498129, 0.9591031389070643, 0.9521792020128582]\n",
    "# score:0.9624190286090247\n",
    "\n",
    "# [\"dating_year_early\", \"dating_year_late\"]のcount encoding ,  lazy_merge_tables_feature\n",
    "# [0.9839995220002874, 0.9605324292607755, 0.9542514170305185, 0.9587683997897447, 0.9483622094005497]\n",
    "# score:0.9612616178598731\n",
    "\n",
    "\"\"\"\n",
    "params2 = params.copy()\n",
    "params2[\"objective\"] = \"regression\"\n",
    "splits = make_stratifirdkfolds(train_data, N_SPLIT, SEED)\n",
    "y_oof, y_pred, scores, feature_importances = run_lgbm(train, y, test, splits, params2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y, np.log1p(y_oof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(12,4))\n",
    "pd.Series(np.log1p(y_oof)).hist(ax=ax[0])\n",
    "pd.Series(y).hist(ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T11:48:29.441033Z",
     "iopub.status.busy": "2020-12-01T11:48:29.440171Z",
     "iopub.status.idle": "2020-12-01T11:48:30.311393Z",
     "shell.execute_reply": "2020-12-01T11:48:30.312068Z"
    },
    "papermill": {
     "duration": 0.996332,
     "end_time": "2020-12-01T11:48:30.312253",
     "exception": false,
     "start_time": "2020-12-01T11:48:29.315921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "DIS = 40\n",
    "order = list(feature_importances[\"feature\"])\n",
    "\n",
    "plt.figure(figsize=(10, DIS * 4 / 10))\n",
    "sns.barplot(x=\"average\", y=\"feature\", data=feature_importances.reset_index().head(DIS), order=order[:DIS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.152663,
     "end_time": "2020-12-01T11:48:46.175096",
     "exception": false,
     "start_time": "2020-12-01T11:48:46.022433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_importances[feature_importances[\"feature\"].str.contains(\"title\", \n",
    "                                                                na=False\n",
    "                                                               )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances[feature_importances[\"average\"] == 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "papermill": {
   "duration": 81.033792,
   "end_time": "2020-12-01T11:48:47.720014",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-01T11:47:26.686222",
   "version": "2.1.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0d44a67646b0433fa5390c47a817b1f3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f23066d5643a4e09a52a2491977f319a",
        "IPY_MODEL_18aa4ee5731841eaa4edbb4b29629d8d"
       ],
       "layout": "IPY_MODEL_f737ce68c65542e5ae24c0ba5ebb9e19"
      }
     },
     "18aa4ee5731841eaa4edbb4b29629d8d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b9477eb23b2d4745ad9f55a6330aa1a4",
       "placeholder": "​",
       "style": "IPY_MODEL_e521816f9fc645b98855f2a2d54c9eca",
       "value": " 100/100 [00:55&lt;00:00,  1.79it/s]"
      }
     },
     "76022b32fd2242258bc0ad95c266a4dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "9923bd079346407cb37a7e6a9c2eac35": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b9477eb23b2d4745ad9f55a6330aa1a4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e521816f9fc645b98855f2a2d54c9eca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f23066d5643a4e09a52a2491977f319a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9923bd079346407cb37a7e6a9c2eac35",
       "max": 100,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_76022b32fd2242258bc0ad95c266a4dc",
       "value": 100
      }
     },
     "f737ce68c65542e5ae24c0ba5ebb9e19": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
