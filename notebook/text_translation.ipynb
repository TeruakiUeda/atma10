{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LhdKAqEf6Xyp"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdZMFrSVYQ-w"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip > /dev/null\n",
    "!pip install transformers > /dev/null\n",
    "!pip3 install sentencepiece > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "taXLwb9YYD9X"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from transformers import BertTokenizer, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "joEhL1RkYMvG"
   },
   "outputs": [],
   "source": [
    "#DATADIR = \"drive/MyDrive/atma10/input/\"\n",
    "#OUTPUTDIR = \"drive/MyDrive/atma10/feature/\"\n",
    "DATADIR = \"../input/\"\n",
    "OUTPUTDIR = \"../feature/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VpWuWwpbYc6j"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(DATADIR + \"train.csv\")\n",
    "test_data = pd.read_csv(DATADIR + \"test.csv\")\n",
    "\n",
    "all_df = pd.concat([train_data, test_data], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4iUSqLJ_9nRe"
   },
   "outputs": [],
   "source": [
    "class BertTranslation:\n",
    "    def __init__(self):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model_name = \"Helsinki-NLP/opus-mt-nl-en\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.bert_model = AutoModelForSeq2SeqLM.from_pretrained(self.model_name)\n",
    "        self.bert_model = self.bert_model.to(self.device)\n",
    "        self.max_len = 128\n",
    "        print(self.device)\n",
    "\n",
    "\n",
    "    def translation(self, sentence : str) -> np.array:\n",
    "        if sentence is np.nan:\n",
    "            return np.nan\n",
    "        input_ids = torch.tensor([self.tokenizer.encode(sentence)], dtype=torch.long)\n",
    "        input_ids = input_ids[:, :512].to(self.device)\n",
    "        #print(input_ids.shape)\n",
    "        output_ids = self.bert_model.generate(input_ids, max_length=512, num_beams=4, early_stopping=True)\n",
    "        trased_text = self.tokenizer.decode(output_ids[0])\n",
    "        return trased_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nFt-cyFj7a0J"
   },
   "outputs": [],
   "source": [
    "# おおよそ2~3 h \n",
    "BT = BertTranslation()\n",
    "\n",
    "for text_col in [\"long_title\", \"description\"]:\n",
    "    c = f\"transed_{text_col}\"\n",
    "    all_df[c] = all_df[c].progress_apply(lambda x: BT.translation(x))\n",
    "    all_df[[\"object_id\", c]].to_csv(OUTPUTDIR + f\"{c}.csv\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMsUnsfgs/GYAvLX/X0sH6d",
   "collapsed_sections": [],
   "mount_file_id": "1RQNVLYMs6Hf44sF0M9Jr6ArHljeuJYEV",
   "name": "text_translation.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
